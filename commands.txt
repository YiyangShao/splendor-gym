# Setup
python -m pip install -e .'[dev]'
python -m pip install cleanrl==0.4.8 torch tensorboard

# Run tests
python -m pytest -q

# Quick random rollout smoke
python -m splendor_gym.scripts.random_rollout --episodes 3

# Train masked PPO with self-play (basic)
python ppo_splendor.py --total-timesteps 2000000 --num-envs 16 --num-steps 128 --gamma 0.999 --ent-coef 0.03 --save-path runs/ppo_splendor.pt

# Train with logging and eval every 10 updates
python ppo_splendor.py --total-timesteps 1000000 --num-envs 16 --num-steps 128 --track --eval-every-updates 10 --eval-games 400 --lr-anneal

# TensorBoard
python -m tensorboard.main --logdir runs

# Evaluate trained model vs random opponent
python -m splendor_gym.scripts.eval_vs_random --games 400 --model runs/ppo_splendor.pt

# Evaluate vs random & greedy_v1 using the suite (interactive Python)
python - << 'PY'
from splendor_gym.scripts.eval_suite import eval_vs_opponent, make_selfplay_env, greedy_opponent_v1
from ppo_splendor import ActorCritic
import torch
from splendor_gym.engine.encode import OBSERVATION_DIM, TOTAL_ACTIONS
agent = ActorCritic(OBSERVATION_DIM, TOTAL_ACTIONS); agent.load_state_dict(torch.load('runs/ppo_splendor.pt', map_location='cpu'))
from splendor_gym.scripts.eval_suite import model_greedy_policy_from
policy = model_greedy_policy_from(agent)
print(eval_vs_opponent(lambda: make_selfplay_env(0)(), policy, n_games=400, seed=42))
print(eval_vs_opponent(lambda: make_selfplay_env(0)(), policy, n_games=400, seed=43))
PY 


python ppo_splendor.py --total-timesteps 60000 --num-envs 16 --num-steps 128 --track --eval-every-updates 10 --eval-games 200 --lr-anneal